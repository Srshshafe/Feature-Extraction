
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML is auto-generated from an M-file.
To make changes, update the M-file and republish this document.
      --><title>HW5_I</title><meta name="generator" content="MATLAB 7.10"><meta name="date" content="2014-03-09"><meta name="m-file" content="HW5_I"><style type="text/css">

body {
  background-color: white;
  margin:10px;
}

h1 {
  color: #990000; 
  font-size: x-large;
}

h2 {
  color: #990000;
  font-size: medium;
}

/* Make the text shrink to fit narrow windows, but not stretch too far in 
wide windows. */ 
p,h1,h2,div.content div {
  max-width: 600px;
  /* Hack for IE6 */
  width: auto !important; width: 600px;
}

pre.codeinput {
  background: #EEEEEE;
  padding: 10px;
}
@media print {
  pre.codeinput {word-wrap:break-word; width:100%;}
} 

span.keyword {color: #0000FF}
span.comment {color: #228B22}
span.string {color: #A020F0}
span.untermstring {color: #B20000}
span.syscmd {color: #B28C00}

pre.codeoutput {
  color: #666666;
  padding: 10px;
}

pre.error {
  color: red;
}

p.footer {
  text-align: right;
  font-size: xx-small;
  font-weight: lighter;
  font-style: italic;
  color: gray;
}

  </style></head><body><div class="content"><h2>Contents</h2><div><ul><li><a href="#2">PCA for both data sets A and B</a></li><li><a href="#3">3</a></li><li><a href="#4">4</a></li><li><a href="#5">5</a></li></ul></div><pre class="codeinput"><span class="comment">%                        Alireza Shafe'</span>
<span class="comment">% No matter how sophisticated a computer,"Coding" is the only way</span>
<span class="comment">% of communicating your thoughts with a computer.</span>
</pre><h2>PCA for both data sets A and B<a name="2"></a></h2><p>Now we don't consider labels ! We just need to reduce the features so we can classify our samples much easier in the future with less features being considered.</p><pre class="codeinput">clc
clear <span class="string">all</span>
close <span class="string">all</span>
Data_a = load(<span class="string">'a.txt'</span>);
Data_b = load(<span class="string">'b.txt'</span>);
Ma = mean(Data_a);
Mb = mean(Data_b);
[row,col] = size(Data_a);
<span class="keyword">for</span> i = 1:row
 Data_a(i,:) = Data_a(i,:)-Ma;
<span class="keyword">end</span>
[row,col] = size(Data_b);
<span class="keyword">for</span> i = 1:row
 Data_b(i,:) = Data_b(i,:)-Mb;
<span class="keyword">end</span>
Sa = cov(Data_a);
Sb = cov(Data_b);
[Ta,Ea] = eig(Sa);
[Tb,Eb] = eig(Sb);
[row,col] = size(Ea);
Ea = Ea*ones(row,1);
[row,col] = size(Eb);
Eb = Eb*ones(row,1);
Ta = double(Ta);
Tb = double(Tb);
Ea = double(Ea);
Eb = double(Eb);
FVa = [];
<span class="keyword">for</span> i=1:length(Ea)
    index = find(max(Ea)==Ea);
    FVa = [FVa,Ta(:,index(1))];
    Ea(index) =[];
    Ta(:,index(1)) =[];
<span class="keyword">end</span>
[row,col] = size(Data_a);
Train_Data_a = [];
<span class="keyword">for</span> i = 1:row
 Train_Data_a(i,:) = Data_a(i,:)*FVa(:,1:2);
<span class="keyword">end</span>
figure(1)
 plot(Train_Data_a(:,1),Train_Data_a(:,2),<span class="string">'.'</span>)
 title([<span class="string">'PCA on data A for 1st and 2nd component'</span>],<span class="string">'Color'</span>,<span class="string">'m'</span>)
 FVb = [];
<span class="keyword">for</span> i=1:length(Eb)
    index = find(max(Eb)==Eb);
    FVb = [FVb,Tb(:,index(1))];
    Eb(index) =[];
    Tb(:,index(1)) =[];
<span class="keyword">end</span>
[row,col] = size(Data_b);
Train_Data_b = [];
<span class="keyword">for</span> i = 1:row
 Train_Data_b(i,:) = Data_b(i,:)*FVb(:,1:2);
<span class="keyword">end</span>
figure(2)
plot(Train_Data_b(:,1),Train_Data_b(:,2),<span class="string">'.'</span>)
 title([<span class="string">'PCA on data B for 1st and 2nd component'</span>],<span class="string">'Color'</span>,<span class="string">'m'</span>)
<span class="comment">% As a result, classification will be done much easier for this Data.</span>
</pre><img vspace="5" hspace="5" src="HW5_I_01.png" alt=""> <img vspace="5" hspace="5" src="HW5_I_02.png" alt=""> <h2>3<a name="3"></a></h2><pre>for 3rd and 4th component</pre><pre class="codeinput">[row,col] = size(Data_a);
Train_Data_a = [];
<span class="keyword">for</span> i = 1:row,
 Train_Data_a(i,:) = Data_a(i,:)*FVa(:,4:5);
<span class="keyword">end</span>
figure(3)
plot(Train_Data_a(:,1),Train_Data_a(:,2),<span class="string">'.'</span>)
 title([<span class="string">'PCA on data A for 3rd and 4th component'</span>],<span class="string">'Color'</span>,<span class="string">'m'</span>)
[row,col] = size(Data_b);
Train_Data_b = [];
<span class="keyword">for</span> i = 1:row
 Train_Data_b(i,:) = Data_b(i,:)*FVb(:,4:5);
<span class="keyword">end</span>
figure(4)
plot(Train_Data_b(:,1),Train_Data_b(:,2),<span class="string">'.'</span>)
 title([<span class="string">'PCA on data B for 3rd and 4th component'</span>],<span class="string">'Color'</span>,<span class="string">'m'</span>)
<span class="comment">%  for 9th and 10th component</span>
[row,col] = size(Data_a);
Train_Data_a = [];
<span class="keyword">for</span> i = 1:row
 Train_Data_a(i,:) = Data_a(i,:)*FVa(:,9:10);
<span class="keyword">end</span>
figure(5)
plot(Train_Data_a(:,1),Train_Data_a(:,2),<span class="string">'.'</span>)
 title([<span class="string">'PCA on data A for 9th and 10th component'</span>],<span class="string">'Color'</span>,<span class="string">'m'</span>)
[row,col] = size(Data_b);
Train_Data_b = [];
<span class="keyword">for</span> i = 1:row
 Train_Data_b(i,:) = Data_b(i,:)*FVb(:,9:10);
<span class="keyword">end</span>
figure(6)
plot(Train_Data_b(:,1),Train_Data_b(:,2),<span class="string">'.'</span>)
 title([<span class="string">'PCA on data B for 9th and 10th component'</span>],<span class="string">'Color'</span>,<span class="string">'m'</span>)
</pre><img vspace="5" hspace="5" src="HW5_I_03.png" alt=""> <img vspace="5" hspace="5" src="HW5_I_04.png" alt=""> <img vspace="5" hspace="5" src="HW5_I_05.png" alt=""> <img vspace="5" hspace="5" src="HW5_I_06.png" alt=""> <h2>4<a name="4"></a></h2><pre class="codeinput"><span class="comment">%  [row,col] = size(Train _Data_b);</span>
<span class="comment">%  rawData_b = [];</span>
<span class="comment">%  for i = 1:row</span>
<span class="comment">%      rawData_b(i,:) = Train_Data_b(i,:)*FVb(:,1:2)';</span>
<span class="comment">%  end</span>
[row,col] = size(Train_Data_a);
rawData_a = [];
<span class="keyword">for</span> i = 1:row
rawData_a(i,:) =  Train_Data_a(i,:)*FVa(:,1:2)';
<span class="keyword">end</span>
Variance_of_Data=var(rawData_a);
Variance_of_first_2_principals_Data=var(Train_Data_a);
Variance_of_Data=var(Variance_of_Data)
Variance_of_first_2_principals_Data=var(Variance_of_first_2_principals_Data)
<span class="comment">% We can see how much of the variance have been retained.</span>
<span class="comment">% There is a reduction in Variance of Data.</span>
</pre><pre class="codeoutput">
Variance_of_Data =

    0.0026


Variance_of_first_2_principals_Data =

    0.0011

</pre><h2>5<a name="5"></a></h2><pre>Here we consider classes</pre><pre class="codeinput">Data = xlsread(<span class="string">'a1.xlt'</span>);
[row,col] = size(Data);
C1 =[];
C2 = [];
C3 =[];
<span class="comment">% again we need to seperate our classes..</span>
<span class="keyword">for</span> i =1:row
    <span class="keyword">if</span> Data(i,1)==1
        C1 = [C1;Data(i,:)];
    <span class="keyword">elseif</span> Data(i,1)==2
        C2 = [C2;Data(i,:)];
    <span class="keyword">else</span>
        C3 = [C3;Data(i,:)];
    <span class="keyword">end</span>
<span class="keyword">end</span>

M1 = mean(C1(:,2:end));
M2 = mean(C2(:,2:end));
M3 = mean(C3(:,2:end));
M = mean(Data(:,2:end));

S1 = cov(C1(:,2:end));
S2 = cov(C2(:,2:end));
S3 = cov(C3(:,2:end));

Sw = S1+S2+S3;
Sb = (M1-M)'*(M1-M)+(M2-M)'*(M2-M)+(M3-M)'*(M3-M);

W = Sw*Sb';
Sb = W;
<span class="comment">% T=&gt; eigenvector , E =&gt; eigenvalue</span>
[Tb,Eb] = eig(Sb);
[row,col] = size(Eb);
Eb = Eb*ones(row,1);
Tb = double(Tb);
Eb = double(Eb);
FVb = [];
<span class="keyword">for</span> i=1:length(Eb)
    index = find(max(Eb)==Eb);
    FVb = [FVb,Tb(:,index(1))];
    Eb(index) =[];
    Tb(:,index(1)) =[];
<span class="keyword">end</span>
[row,col] = size(C1);
Train_Data_b = [];
<span class="keyword">for</span> i = 1:row
 Train_Data_b(i,:) = C1(i,2:end)*FVb(:,1:2);
<span class="keyword">end</span>
figure(7)
plot(Train_Data_b(:,1),Train_Data_b(:,2),<span class="string">'.'</span>)
 title([<span class="string">'Dimension Reduction using Fisher discriminant Analysis for Class 1'</span>],<span class="string">'Color'</span>,<span class="string">'m'</span>)

[row,col] = size(C2);
Train_Data_b = [];
<span class="keyword">for</span> i = 1:row
 Train_Data_b(i,:) = C2(i,2:end)*FVb(:,1:2);
<span class="keyword">end</span>
figure(8)
plot(Train_Data_b(:,1),Train_Data_b(:,2),<span class="string">'.'</span>)
 title([<span class="string">'Dimension Reduction using Fisher discriminant Analysis for Class 2'</span>],<span class="string">'Color'</span>,<span class="string">'m'</span>)

[row,col] = size(C3);
Train_Data_b = [];
<span class="keyword">for</span> i = 1:row
 Train_Data_b(i,:) = C3(i,2:end)*FVb(:,1:2);
<span class="keyword">end</span>
figure(9)
plot(Train_Data_b(:,1),Train_Data_b(:,2),<span class="string">'.'</span>)
 title([<span class="string">'Dimension Reduction using Fisher discriminant Analysis for Class 3'</span>],<span class="string">'Color'</span>,<span class="string">'m'</span>)
</pre><img vspace="5" hspace="5" src="HW5_I_07.png" alt=""> <img vspace="5" hspace="5" src="HW5_I_08.png" alt=""> <img vspace="5" hspace="5" src="HW5_I_09.png" alt=""> <p class="footer"><br>
      Published with MATLAB&reg; 7.10<br></p></div><!--
##### SOURCE BEGIN #####
%                        Alireza Shafe'
% No matter how sophisticated a computer,"Coding" is the only way
% of communicating your thoughts with a computer.
%% PCA for both data sets A and B
% Now we don't consider labels ! 
% We just need to reduce the features so we can classify our samples
% much easier in the future with less features being considered.
clc
clear all
close all
Data_a = load('a.txt');
Data_b = load('b.txt');
Ma = mean(Data_a);
Mb = mean(Data_b);
[row,col] = size(Data_a);
for i = 1:row
 Data_a(i,:) = Data_a(i,:)-Ma;
end
[row,col] = size(Data_b);
for i = 1:row
 Data_b(i,:) = Data_b(i,:)-Mb;
end
Sa = cov(Data_a);
Sb = cov(Data_b);
[Ta,Ea] = eig(Sa);
[Tb,Eb] = eig(Sb);
[row,col] = size(Ea);
Ea = Ea*ones(row,1);
[row,col] = size(Eb);
Eb = Eb*ones(row,1);
Ta = double(Ta);
Tb = double(Tb);
Ea = double(Ea);
Eb = double(Eb);
FVa = [];
for i=1:length(Ea)
    index = find(max(Ea)==Ea);
    FVa = [FVa,Ta(:,index(1))];
    Ea(index) =[];
    Ta(:,index(1)) =[];
end
[row,col] = size(Data_a);
Train_Data_a = [];
for i = 1:row
 Train_Data_a(i,:) = Data_a(i,:)*FVa(:,1:2);
end
figure(1)
 plot(Train_Data_a(:,1),Train_Data_a(:,2),'.')
 title(['PCA on data A for 1st and 2nd component'],'Color','m')
 FVb = [];
for i=1:length(Eb)
    index = find(max(Eb)==Eb);
    FVb = [FVb,Tb(:,index(1))];
    Eb(index) =[];
    Tb(:,index(1)) =[];
end
[row,col] = size(Data_b);
Train_Data_b = [];
for i = 1:row
 Train_Data_b(i,:) = Data_b(i,:)*FVb(:,1:2);
end
figure(2)
plot(Train_Data_b(:,1),Train_Data_b(:,2),'.')
 title(['PCA on data B for 1st and 2nd component'],'Color','m')
% As a result, classification will be done much easier for this Data.

%% 3
%  for 3rd and 4th component
[row,col] = size(Data_a);
Train_Data_a = [];
for i = 1:row,
 Train_Data_a(i,:) = Data_a(i,:)*FVa(:,4:5);
end
figure(3)
plot(Train_Data_a(:,1),Train_Data_a(:,2),'.')
 title(['PCA on data A for 3rd and 4th component'],'Color','m')
[row,col] = size(Data_b);
Train_Data_b = [];
for i = 1:row
 Train_Data_b(i,:) = Data_b(i,:)*FVb(:,4:5);
end
figure(4)
plot(Train_Data_b(:,1),Train_Data_b(:,2),'.')
 title(['PCA on data B for 3rd and 4th component'],'Color','m')
%  for 9th and 10th component
[row,col] = size(Data_a);
Train_Data_a = [];
for i = 1:row
 Train_Data_a(i,:) = Data_a(i,:)*FVa(:,9:10);
end
figure(5)
plot(Train_Data_a(:,1),Train_Data_a(:,2),'.')
 title(['PCA on data A for 9th and 10th component'],'Color','m')
[row,col] = size(Data_b);
Train_Data_b = [];
for i = 1:row
 Train_Data_b(i,:) = Data_b(i,:)*FVb(:,9:10);
end
figure(6)
plot(Train_Data_b(:,1),Train_Data_b(:,2),'.')
 title(['PCA on data B for 9th and 10th component'],'Color','m')
 %% 4
%  [row,col] = size(Train _Data_b); 
%  rawData_b = [];
%  for i = 1:row
%      rawData_b(i,:) = Train_Data_b(i,:)*FVb(:,1:2)';
%  end
[row,col] = size(Train_Data_a);
rawData_a = [];
for i = 1:row
rawData_a(i,:) =  Train_Data_a(i,:)*FVa(:,1:2)';
end
Variance_of_Data=var(rawData_a);
Variance_of_first_2_principals_Data=var(Train_Data_a);
Variance_of_Data=var(Variance_of_Data)
Variance_of_first_2_principals_Data=var(Variance_of_first_2_principals_Data)
% We can see how much of the variance have been retained.
% There is a reduction in Variance of Data.
%% 5
%  Here we consider classes
Data = xlsread('a1.xlt');
[row,col] = size(Data);
C1 =[];
C2 = [];
C3 =[];
% again we need to seperate our classes..
for i =1:row
    if Data(i,1)==1
        C1 = [C1;Data(i,:)];
    elseif Data(i,1)==2
        C2 = [C2;Data(i,:)];
    else
        C3 = [C3;Data(i,:)];
    end
end

M1 = mean(C1(:,2:end));
M2 = mean(C2(:,2:end));
M3 = mean(C3(:,2:end));
M = mean(Data(:,2:end));

S1 = cov(C1(:,2:end));
S2 = cov(C2(:,2:end));
S3 = cov(C3(:,2:end));

Sw = S1+S2+S3;
Sb = (M1-M)'*(M1-M)+(M2-M)'*(M2-M)+(M3-M)'*(M3-M);

W = Sw*Sb';
Sb = W;
% T=> eigenvector , E => eigenvalue
[Tb,Eb] = eig(Sb);
[row,col] = size(Eb);
Eb = Eb*ones(row,1);
Tb = double(Tb);
Eb = double(Eb);
FVb = [];
for i=1:length(Eb)
    index = find(max(Eb)==Eb);
    FVb = [FVb,Tb(:,index(1))];
    Eb(index) =[];
    Tb(:,index(1)) =[];
end
[row,col] = size(C1);
Train_Data_b = [];
for i = 1:row
 Train_Data_b(i,:) = C1(i,2:end)*FVb(:,1:2);
end
figure(7)
plot(Train_Data_b(:,1),Train_Data_b(:,2),'.')
 title(['Dimension Reduction using Fisher discriminant Analysis for Class 1'],'Color','m')

[row,col] = size(C2);
Train_Data_b = [];
for i = 1:row
 Train_Data_b(i,:) = C2(i,2:end)*FVb(:,1:2);
end
figure(8)
plot(Train_Data_b(:,1),Train_Data_b(:,2),'.')
 title(['Dimension Reduction using Fisher discriminant Analysis for Class 2'],'Color','m')

[row,col] = size(C3);
Train_Data_b = [];
for i = 1:row
 Train_Data_b(i,:) = C3(i,2:end)*FVb(:,1:2);
end
figure(9)
plot(Train_Data_b(:,1),Train_Data_b(:,2),'.')
 title(['Dimension Reduction using Fisher discriminant Analysis for Class 3'],'Color','m')
 

##### SOURCE END #####
--></body></html>